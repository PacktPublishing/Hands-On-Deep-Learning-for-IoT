{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: IoT Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case 2: Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Autoencoder (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Collection (downloading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this use case we are using famus opensource KDDCUP99 data. Please go to this link http://kdd.ics.uci.edu/databases/kddcup99/ and download kddcup.data_10_percent.gz dataset, and save into your preferred folder for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Pre-processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KDD IDS dataset needs multi-level pre-processing: (i) splitting the data into three different protocol sets ( Application, Transport, and Network ) in order extract the dataset for the desired protocol (ii) duplicate data removal, categorical data conversion and normalisation, and (iii) feature selection (optional). The following lines of code is a potential way of splitting the dataset into three datasets, namely 'Final_App_Layer’, 'Final_Transport_Layer’, and 'Final_Network_Layer’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Entire Data\n",
    "\n",
    "#Importing all the required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "IDSdata = pd.read_csv(\"kddcup.data_10_percent.csv\",header = None,engine = 'python',sep=\",\")\n",
    "\n",
    "# Add column header \n",
    "\n",
    "IDSdata.columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragement\",\n",
    "                   \"urgent\",\n",
    "                  \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compressed\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "                   \"num_file_creations\",\n",
    "                  \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "                  \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\n",
    "                   \"srv_diff_host_rate\",\"dst_host_count\",\n",
    "                  \"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "                   \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\n",
    "                  \"dst_host_srv_rerror_rate\",\"labels\"]\n",
    "\n",
    "# Explore the Application Layer IDS Data\n",
    "\n",
    "ApplicationLayer = IDSdata[(IDSdata['labels'].isin(['normal.','smurf.','back.','satan.','pod.','guess_passwd.','buffer_overflow.',\n",
    "                                                    'warezmaster.','imap.','loadmodule.','ftp_write.','multihop.','perl.']))]\n",
    "print (ApplicationLayer['labels'].value_counts())\n",
    "\n",
    "# Save a Applayer data only into a text file\n",
    "ApplicationLayer.to_csv('Final_App_Layer.txt',header = None,index = False)\n",
    "\n",
    "# Explore the Transport Layer IDS Data\n",
    "TransportLayer = IDSdata[(IDSdata['labels'].isin(['normal.','neptune.','portsweep.','teardrop.','buffer_overflow.',\n",
    "                                                 'land.','nmap.']))]\n",
    "print (TransportLayer['labels'].value_counts())\n",
    "TransportLayer.to_csv('Final_Transport_Layer.txt',header = None,index = False)\n",
    "\n",
    "# Explore the Network Layer IDS Data\n",
    "NetworkLayer = IDSdata[(IDSdata['labels'].isin(['normal.','smurf.','ipsweep.','pod.','buffer_overflow.']))]\n",
    "print (NetworkLayer['labels'].value_counts())\n",
    "NetworkLayer.to_csv('Final_Network_Layer.txt',header = None,index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate data removal, categorical data conversion and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laoding the IDS Data\n",
      "Loading the data\n",
      "Data Preprocessing of loaded IDS Data\n",
      "Original number of records in the training dataset before removing duplicates is:  379609\n",
      "Number of records in the training dataset after removing the duplicates is : 89360 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "## For OMP error\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Functions for DataLoading and pre-processing\n",
    "\n",
    "def DataLoading (mypath):\n",
    "    print (\"Loading the data\")\n",
    "    dataframe = pd.read_csv(mypath,header = None,engine = 'python',sep=\",\")\n",
    "    return dataframe\n",
    "\n",
    "def DataPreprocessing(mydataframe):\n",
    "    \n",
    "    # Dropping the duplicates\n",
    "    recordcount = len(mydataframe)\n",
    "    print (\"Original number of records in the training dataset before removing duplicates is: \" , recordcount)\n",
    "    mydataframe.drop_duplicates(subset=None, inplace=True)  # Python command to drop duplicates\n",
    "    newrecordcount = len(mydataframe)\n",
    "    print (\"Number of records in the training dataset after removing the duplicates is :\", newrecordcount,\"\\n\")\n",
    "\n",
    "    #Dropping the labels to a different dataset which is used to train the recurrent neural network classifier\n",
    "    df_X = mydataframe.drop(mydataframe.columns[41],axis=1,inplace = False)\n",
    "    df_Y = mydataframe.drop(mydataframe.columns[0:41],axis=1, inplace = False)\n",
    "\n",
    "    # Convert Categorial data to the numerical data for the efficient classification\n",
    "    df_X[df_X.columns[1:4]] = df_X[df_X.columns[1:4]].stack().rank(method='dense').unstack()\n",
    "    \n",
    "    # Coding the normal as \" 1 0\" and attack as \"0 1\"\n",
    "    df_Y[df_Y[41]!='normal.'] = 0\n",
    "    df_Y[df_Y[41]=='normal.'] = 1\n",
    "    #print (labels[41].value_counts())\n",
    "    \n",
    "    #converting input data into float which is requried in the future stage of building in the network\n",
    "    df_X = df_X.loc[:,df_X.columns[0:41]].astype(float)\n",
    "\n",
    "    # Normal is \"1 0\" and the abnormal is \"0 1\"\n",
    "    df_Y.columns = [\"y1\"]\n",
    "    df_Y.loc[:,('y2')] = df_Y['y1'] ==0\n",
    "    df_Y.loc[:,('y2')] = df_Y['y2'].astype(int)\n",
    "    \n",
    "    return df_X,df_Y\n",
    "\n",
    "print (\"Laoding the IDS Data\")\n",
    "#data_path = \"Final_App_Layer.txt\" # If you want to use for Applicaiton Layer\n",
    "#data_path = \"Final_Transport_Layer.txt\" # If you want to use for Transport Layer\n",
    "data_path = \"Final_Network_Layer.txt\" # If you want to use for Network Layer\n",
    "\n",
    "dataframe = DataLoading(data_path)\n",
    "\n",
    "print (\"Data Preprocessing of loaded IDS Data\")\n",
    "data_X, data_Y = DataPreprocessing(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection (optional pre-processing, but useful for network size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing the Feature Selection on the train data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raz/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "##### Function for features selection for the Model Training\n",
    "\n",
    "def FeatureSelection(myinputX, myinputY):\n",
    "\n",
    "    labels = np.array(myinputY).astype(int)\n",
    "    inputX = np.array(myinputX)\n",
    "    \n",
    "    #Random Forest Model\n",
    "    model = RandomForestClassifier(random_state = 0)\n",
    "    model.fit(inputX,labels)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    \n",
    "    #Plotting the Features agains their importance scores\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.title(\"Feature importances (y-axis) vs Features IDs(x-axis)\")\n",
    "    plt.bar(range(inputX.shape[1]), importances[indices],\n",
    "       color=\"b\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(inputX.shape[1]), indices)\n",
    "    plt.xlim([-1, inputX.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "    # Selecting top featueres which have higher importance values = here we can find \"12\" features\n",
    "    #as we can see in the next step\n",
    "    newX = myinputX.iloc[:,model.feature_importances_.argsort()[::-1][:12]]\n",
    "   # Converting the X dataframe into tensors\n",
    "    myX = newX.as_matrix()\n",
    "    myY = labels\n",
    "\n",
    "    return myX,myY\n",
    "\n",
    "## Visualise the data for feature selction\n",
    "    \n",
    "\n",
    "print (\"Performing the Feature Selection on the train data set\")\n",
    "reduced_X,reduced_Y = FeatureSelection(data_X,data_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape is : (8000, 12)\n",
      "Train Y shape is : (8000, 2)\n",
      "Test X shape is : (1999, 12)\n",
      "Test Y shape is : (1999, 2)\n",
      "WARNING:tensorflow:From /Users/raz/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/raz/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8000 samples, validate on 1999 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.1640 - acc: 0.4454 - val_loss: 0.1481 - val_acc: 0.8939\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0901 - acc: 0.7556 - val_loss: 0.1449 - val_acc: 0.6353\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 0.0835 - acc: 0.7321 - val_loss: 0.1356 - val_acc: 0.7109\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.0811 - acc: 0.7331 - val_loss: 0.1286 - val_acc: 0.9475\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.0802 - acc: 0.7672 - val_loss: 0.1306 - val_acc: 0.9160\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0796 - acc: 0.7590 - val_loss: 0.1329 - val_acc: 0.9260\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0793 - acc: 0.7671 - val_loss: 0.1349 - val_acc: 0.9085\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0790 - acc: 0.7819 - val_loss: 0.1361 - val_acc: 0.9260\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0788 - acc: 0.7980 - val_loss: 0.1353 - val_acc: 0.9155\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0786 - acc: 0.7920 - val_loss: 0.1356 - val_acc: 0.8869\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0785 - acc: 0.7917 - val_loss: 0.1366 - val_acc: 0.7474\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0783 - acc: 0.7876 - val_loss: 0.1343 - val_acc: 0.7374\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0783 - acc: 0.8061 - val_loss: 0.1348 - val_acc: 0.7059\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0782 - acc: 0.7950 - val_loss: 0.1318 - val_acc: 0.6808\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0781 - acc: 0.7937 - val_loss: 0.1324 - val_acc: 0.6433\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0780 - acc: 0.8024 - val_loss: 0.1330 - val_acc: 0.7289\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0780 - acc: 0.8105 - val_loss: 0.1322 - val_acc: 0.8874\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0779 - acc: 0.7892 - val_loss: 0.1269 - val_acc: 0.6303\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.0778 - acc: 0.8016 - val_loss: 0.1243 - val_acc: 0.7164\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.0778 - acc: 0.7984 - val_loss: 0.1247 - val_acc: 0.6378\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 0.0778 - acc: 0.8074 - val_loss: 0.1235 - val_acc: 0.8924\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0468 - acc: 0.7282 - val_loss: 0.0497 - val_acc: 0.8554\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0028 - acc: 0.6584 - val_loss: 0.0413 - val_acc: 0.6218\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0020 - acc: 0.5569 - val_loss: 0.0472 - val_acc: 0.6153\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0018 - acc: 0.5650 - val_loss: 0.0477 - val_acc: 0.6233\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0017 - acc: 0.5584 - val_loss: 0.0469 - val_acc: 0.6233\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0017 - acc: 0.5687 - val_loss: 0.0474 - val_acc: 0.6173\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0017 - acc: 0.5318 - val_loss: 0.0459 - val_acc: 0.6253\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0016 - acc: 0.5470 - val_loss: 0.0457 - val_acc: 0.6338\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.0015 - acc: 0.5499 - val_loss: 0.0454 - val_acc: 0.8929\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.0015 - acc: 0.5516 - val_loss: 0.0412 - val_acc: 0.8199\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0015 - acc: 0.5319 - val_loss: 0.0426 - val_acc: 0.9010\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0015 - acc: 0.5468 - val_loss: 0.0425 - val_acc: 0.6918\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0015 - acc: 0.5295 - val_loss: 0.0432 - val_acc: 0.8939\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.0014 - acc: 0.5405 - val_loss: 0.0440 - val_acc: 0.7439\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0014 - acc: 0.5361 - val_loss: 0.0430 - val_acc: 0.7399\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0014 - acc: 0.5246 - val_loss: 0.0458 - val_acc: 0.8654\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 91us/step - loss: 0.0013 - acc: 0.5404 - val_loss: 0.0430 - val_acc: 0.7844\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.0013 - acc: 0.5295 - val_loss: 0.0435 - val_acc: 0.8049\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.0013 - acc: 0.5034 - val_loss: 0.0466 - val_acc: 0.8264\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.0013 - acc: 0.5273 - val_loss: 0.0480 - val_acc: 0.8054\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.0012 - acc: 0.5201 - val_loss: 0.0445 - val_acc: 0.5758\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.0012 - acc: 0.5363 - val_loss: 0.0451 - val_acc: 0.7994\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0012 - acc: 0.5397 - val_loss: 0.0445 - val_acc: 0.8104\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.0012 - acc: 0.5560 - val_loss: 0.0479 - val_acc: 0.8204\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.0011 - acc: 0.5196 - val_loss: 0.0425 - val_acc: 0.4722\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.0012 - acc: 0.5289 - val_loss: 0.0432 - val_acc: 0.6303\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.0012 - acc: 0.5076 - val_loss: 0.0449 - val_acc: 0.5443\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.0011 - acc: 0.5280 - val_loss: 0.0439 - val_acc: 0.4437\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.0011 - acc: 0.5231 - val_loss: 0.0469 - val_acc: 0.4972\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.0011 - acc: 0.5349 - val_loss: 0.0418 - val_acc: 0.5148\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 0.0011 - acc: 0.5590 - val_loss: 0.0454 - val_acc: 0.4912\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.0011 - acc: 0.5240 - val_loss: 0.0424 - val_acc: 0.5458\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.0011 - acc: 0.5520 - val_loss: 0.0454 - val_acc: 0.4402\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.0010 - acc: 0.5071 - val_loss: 0.0442 - val_acc: 0.1921\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.0010 - acc: 0.5281 - val_loss: 0.0437 - val_acc: 0.2326\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 9.9950e-04 - acc: 0.5487 - val_loss: 0.0444 - val_acc: 0.3712\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.0010 - acc: 0.5384 - val_loss: 0.0448 - val_acc: 0.3217\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.0010 - acc: 0.5080 - val_loss: 0.0427 - val_acc: 0.1701\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 9.7716e-04 - acc: 0.5395 - val_loss: 0.0429 - val_acc: 0.1661\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 9.5707e-04 - acc: 0.5172 - val_loss: 0.0443 - val_acc: 0.1461\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 9.9759e-04 - acc: 0.5410 - val_loss: 0.0425 - val_acc: 0.1286\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 9.7936e-04 - acc: 0.5541 - val_loss: 0.0453 - val_acc: 0.2046\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 9.3832e-04 - acc: 0.5051 - val_loss: 0.0420 - val_acc: 0.1841\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.0010 - acc: 0.5426 - val_loss: 0.0389 - val_acc: 0.1391\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 9.1420e-04 - acc: 0.5184 - val_loss: 0.0423 - val_acc: 0.1631\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 9.0200e-04 - acc: 0.5359 - val_loss: 0.0422 - val_acc: 0.1546\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 9.5729e-04 - acc: 0.5428 - val_loss: 0.0378 - val_acc: 0.1636\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 8.7990e-04 - acc: 0.5351 - val_loss: 0.0362 - val_acc: 0.1736\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.0010 - acc: 0.5296 - val_loss: 0.0417 - val_acc: 0.2746\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 8.5644e-04 - acc: 0.5229 - val_loss: 0.0428 - val_acc: 0.3717\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 8.7562e-04 - acc: 0.5389 - val_loss: 0.0434 - val_acc: 0.3042\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 8.5456e-04 - acc: 0.5291 - val_loss: 0.0419 - val_acc: 0.1806\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 8.7505e-04 - acc: 0.5121 - val_loss: 0.0443 - val_acc: 0.1996\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 8.5422e-04 - acc: 0.5213 - val_loss: 0.0420 - val_acc: 0.1676\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 9.1479e-04 - acc: 0.5376 - val_loss: 0.0421 - val_acc: 0.4457\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 8.2786e-04 - acc: 0.5321 - val_loss: 0.0413 - val_acc: 0.1936\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 8.3750e-04 - acc: 0.5225 - val_loss: 0.0414 - val_acc: 0.1906\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 8.9597e-04 - acc: 0.5410 - val_loss: 0.0425 - val_acc: 0.1616\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 8.0489e-04 - acc: 0.5090 - val_loss: 0.0400 - val_acc: 0.1591\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 7.8469e-04 - acc: 0.5486 - val_loss: 0.0404 - val_acc: 0.1206\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 8.6167e-04 - acc: 0.5187 - val_loss: 0.0459 - val_acc: 0.1906\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 8.4289e-04 - acc: 0.5208 - val_loss: 0.0376 - val_acc: 0.1391\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 7.8791e-04 - acc: 0.5272 - val_loss: 0.0404 - val_acc: 0.2386\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 7.9852e-04 - acc: 0.5175 - val_loss: 0.0456 - val_acc: 0.0970\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 8.2332e-04 - acc: 0.5560 - val_loss: 0.0409 - val_acc: 0.1521\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 7.6738e-04 - acc: 0.5331 - val_loss: 0.0394 - val_acc: 0.1211\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 8.3813e-04 - acc: 0.5376 - val_loss: 0.0395 - val_acc: 0.1526\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 7.7671e-04 - acc: 0.5299 - val_loss: 0.0397 - val_acc: 0.1541\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 8.0075e-04 - acc: 0.5232 - val_loss: 0.0420 - val_acc: 0.2451\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 7.5247e-04 - acc: 0.5214 - val_loss: 0.0395 - val_acc: 0.1451\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 9.1716e-04 - acc: 0.5399 - val_loss: 0.0357 - val_acc: 0.2401\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 7.5186e-04 - acc: 0.5299 - val_loss: 0.0384 - val_acc: 0.1371\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 7.3044e-04 - acc: 0.5089 - val_loss: 0.0383 - val_acc: 0.1531\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 7.4793e-04 - acc: 0.5425 - val_loss: 0.0364 - val_acc: 0.1266\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 7.7556e-04 - acc: 0.5355 - val_loss: 0.0359 - val_acc: 0.2346\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 7.4365e-04 - acc: 0.5476 - val_loss: 0.0348 - val_acc: 0.1866\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 7.3507e-04 - acc: 0.5362 - val_loss: 0.0349 - val_acc: 0.1981\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 8.3712e-04 - acc: 0.5181 - val_loss: 0.0356 - val_acc: 0.2016\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 7.0996e-04 - acc: 0.5007 - val_loss: 0.0373 - val_acc: 0.1611\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "\n",
    "# Train features and Train Labels\n",
    "train_X = reduced_X[:8000]\n",
    "train_Y = reduced_Y[:8000]\n",
    "\n",
    "#Test Features and Test Labels\n",
    "test_X = reduced_X[8001:10000]\n",
    "test_Y = reduced_Y[8001:10000]\n",
    "\n",
    "\n",
    "print (\"Train X shape is :\", train_X.shape)\n",
    "print (\"Train Y shape is :\", train_Y.shape)\n",
    "print (\"Test X shape is :\", test_X.shape)\n",
    "print (\"Test Y shape is :\", test_Y.shape)\n",
    "\n",
    "## Normalizing the Input Features: Using tensorflow normalizing function\n",
    "\n",
    "# Before normalizing, the array of input features should be converted to a dataframe\n",
    "semitrain_X = pd.DataFrame(train_X)\n",
    "semitest_X = pd.DataFrame(test_X)\n",
    "#Importing Scikit learn libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#Normalizing Train Data Features\n",
    "scaler_traindata = scaler.fit(semitrain_X)\n",
    "train_norm = scaler_traindata.transform(semitrain_X)\n",
    "X_train=train_norm_X = pd.DataFrame(train_norm)\n",
    "\n",
    "#Normalizing Test Data Features\n",
    "scaler_testdata = scaler.fit(semitest_X)\n",
    "test_norm = scaler_testdata.transform(semitest_X)\n",
    "X_test=test_norm_X = pd.DataFrame(test_norm)\n",
    "#Testing/Training Data\n",
    "Y_train=train_Y\n",
    "Y_test=test_Y\n",
    "# Useful paprameters for the Autoencoder\n",
    " # dimension one one input data\n",
    "input_dim = X_train.shape[1]\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32 \n",
    "    \n",
    "## For OMP error\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim), activation=\"relu\")(encoder)\n",
    "encoder = Dense(int(encoding_dim-2), activation=\"relu\")(encoder)\n",
    "code = Dense(int(encoding_dim-4), activation='tanh')(encoder)\n",
    "decoder = Dense(int(encoding_dim-2), activation='tanh')(code)\n",
    "decoder = Dense(int(encoding_dim), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "## Parameters for Model configuration and saving \n",
    "nb_epoch = 100\n",
    "batch_size = 60\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "                               \n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history\n",
    "                                                \n",
    "\n",
    "autoencoder = load_model('model.h5')\n",
    "predictions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
